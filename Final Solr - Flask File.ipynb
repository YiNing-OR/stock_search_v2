{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c264c51b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install flask-cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c24bdc3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#!pip install Flask-Session\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "872291a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7605b58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "14f91210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# spacy.cli.download(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c8384f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install symspellpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c1dfc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "061497ff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#pip install flair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2fa9c413",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/waichung/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib.request\n",
    "from urllib.request import Request, urlopen\n",
    "from random import randint\n",
    "from time import sleep\n",
    "import multiprocess as mp\n",
    "from multiprocess.dummy import Pool\n",
    "import tqdm as tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from requests.exceptions import SSLError,ChunkedEncodingError\n",
    "import re\n",
    "from datetime import datetime as dt\n",
    "import pysolr\n",
    "from textblob import TextBlob\n",
    "import flair\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import spacy\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "stopword = stopwords.words('english')\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "daf769a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest_date(solr):\n",
    "    connection = urllib.request.urlopen('http://ec2-3-132-215-195.us-east-2.compute.amazonaws.com:8983/solr/admin/cores?wt=xml')\n",
    "    response = list(connection)\n",
    "    last_mod = [str(x) for x in response if 'lastModified' in str(x)][0]\n",
    "    result = re.search('%s(.*)%s' % ('>', '<'), last_mod).group(1)\n",
    "    dt_obj=dt.strptime(result,\"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
    "    return dt_obj\n",
    "\n",
    "def random_proxy(proxies):\n",
    "    return proxies[randint(0, len(proxies) - 1)]\n",
    "my_headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\",  \n",
    "              \"Accept\":\"text/html,application/xhtml+xml,application/xml; q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\", \n",
    "              \"Accept-Encoding\": \"gzip\", \n",
    "              \"Accept-Language\": \"en-US,en;q=0.9,es;q=0.8\", \n",
    "              \"Upgrade-Insecure-Requests\": \"1\",}\n",
    "def get_proxies():\n",
    "    proxies = []\n",
    "    proxies_req = Request('https://www.sslproxies.org/')\n",
    "    proxies_req.add_header('User-Agent', my_headers['User-Agent'])\n",
    "    proxies_doc = urlopen(proxies_req).read().decode('utf8')\n",
    "    proxies_list = str(BeautifulSoup(proxies_doc, 'html.parser').find_all(\"textarea\", class_=\"form-control\")[0]).split('\\n')[3:-1]\n",
    "    return proxies_list\n",
    "\n",
    "def update_url_list(prox,latest_date):\n",
    "    def get_urls(number):\n",
    "        page_urls=[]\n",
    "        date = []\n",
    "        author =[]\n",
    "        url = 'https://www.fool.ca/recent-headlines/page/' + str(number) + '/'\n",
    "        number+=1\n",
    "        proxy = random_proxy(prox)\n",
    "        session = requests.Session()\n",
    "        session.proxies = {'http': 'http://'+proxy}\n",
    "        page = session.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        for a in soup.find(\"main\").findAll('a', href=True):\n",
    "            for c in a.findAll(\"time\" ):\n",
    "                b=c['datetime'][:-6]\n",
    "                d=dt.strptime(b,\"%Y-%m-%dT%H:%M:%S\")  \n",
    "                if d>latest_date:\n",
    "                    if 'page' not in str(a['href']):\n",
    "                        page_urls.append([(a['href']),b+'Z'])\n",
    "                else:\n",
    "                    number=-1\n",
    "        return page_urls,number\n",
    "\n",
    "    counter=0\n",
    "    allurl=[]\n",
    "    while counter>=0:\n",
    "        page_url,counter=get_urls(counter)\n",
    "        allurl.append(page_url)\n",
    "    all_url=[item for sublist in allurl for item in sublist]\n",
    "    return all_url\n",
    "\n",
    "def update_content_df(prox,all_url):\n",
    "    def get_info(url_lst):\n",
    "        url,time=url_lst\n",
    "        my_headers = {\"User-Agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.102 Safari/537.36\",  \n",
    "                  \"Accept\":\"text/html,application/xhtml+xml,application/xml; q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3\", \n",
    "                  \"Accept-Encoding\": \"gzip\", \n",
    "                  \"Accept-Language\": \"en-US,en;q=0.9,es;q=0.8\", \n",
    "                  \"Upgrade-Insecure-Requests\": \"1\",}\n",
    "        counter = 0\n",
    "        #url = 'https://www.fool.com.au/2022/03/05/5-asx-shares-making-big-news-this-week/'\n",
    "        response = Request(url , headers=my_headers)\n",
    "        proxy = random_proxy(prox)\n",
    "        session = requests.Session()\n",
    "        session.proxies = {'http': 'http://'+proxy}\n",
    "        while True:\n",
    "            try:\n",
    "                page = session.get(url, headers=my_headers)\n",
    "                break\n",
    "            except (SSLError, ConnectionResetError, ChunkedEncodingError):\n",
    "                proxy = random_proxy(prox)\n",
    "                session = requests.Session()\n",
    "                session.proxies = {'http': 'http://'+proxy}\n",
    "                counter+=1\n",
    "                if counter==10:\n",
    "                    break\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        #             page = session.get(url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        soup = BeautifulSoup(page.content, \"html.parser\")\n",
    "        stocks_mentioned = soup.find_all(class_=\"chicklets\")\n",
    "        if len(stocks_mentioned)>0:\n",
    "            stocks_mentioned = [i.text for i in stocks_mentioned[0].find_all('a')]\n",
    "        key_points = soup.find_all(id='key-points')\n",
    "        if len(key_points)>0:\n",
    "            key_points = [i.text for i in key_points[0].find_all('li')]\n",
    "        info={\"title\":[i.text for i in soup.findAll('title')][0],\n",
    "              \"author\":[i.text for i in soup.findAll(class_='url fn n')][0],\n",
    "              'date_published':url.split('/')[3]+'/'+url.split('/')[4]+'/'+url.split('/')[5],\n",
    "              \"content\":[a.text for a in soup.find_all(class_='entry-content')[0].find_all('p')], \n",
    "              \"stocks_mentioned\": stocks_mentioned,\n",
    "              'key_points':key_points,\n",
    "             'url':url} \n",
    "    #     time.sleep(5)\n",
    "        return info\n",
    "\n",
    "    pool = Pool(processes=8)\n",
    "    mapped_values = list(tqdm.tqdm(pool.imap_unordered(get_info, all_url), total=len(all_url)))\n",
    "    df2=pd.DataFrame(mapped_values)\n",
    "    return df2\n",
    "\n",
    "class SentimentAnalysis():\n",
    "    def __init__(self, list_of_sentences):\n",
    "        self.list_of_sentences = list_of_sentences\n",
    "        self.sentiment_vader = SentimentIntensityAnalyzer()\n",
    "        self.sentiment_textblob = TextBlob\n",
    "        self.sentiment_flair = flair.models.TextClassifier.load('en-sentiment')\n",
    "    def get_vader_sentiment(self, sentence):\n",
    "        try:\n",
    "            self.sentiment_vader.polarity_scores(sentence)['compound']\n",
    "        except:\n",
    "            print(sentence)\n",
    "        return self.sentiment_vader.polarity_scores(sentence)['compound']\n",
    "    def get_textblob_sentiment(self, sentence):\n",
    "        result = self.sentiment_textblob(sentence).sentiment\n",
    "        return ((4-result.subjectivity)/4) * result.polarity\n",
    "    def get_flair_sentiment(self, sentence):\n",
    "        result = flair.data.Sentence(sentence)\n",
    "        self.sentiment_flair.predict(result)\n",
    "        return result.labels[0].score*(-1 if result.labels[0].value == 'NEGATIVE' else 1)\n",
    "    def get_average_sentiment(self, sentence):\n",
    "        return (self.get_vader_sentiment(sentence) + self.get_textblob_sentiment(sentence) + self.get_flair_sentiment(sentence))/3\n",
    "    def get_weighted_sentiment(self, sentence):\n",
    "        vader = self.get_vader_sentiment(sentence)\n",
    "        vader_positive = vader>0.0\n",
    "        textblob = self.get_textblob_sentiment(sentence)\n",
    "        textblob_positive = textblob>0.0\n",
    "        flair = self.get_flair_sentiment(sentence)\n",
    "        flair_positive = flair>0.0\n",
    "        if (vader==0.0 or textblob==0.0 or flair==0.0):\n",
    "            # assumption that only one model gives us 0.0 prediction\n",
    "            return (self.get_vader_sentiment(sentence) + self.get_textblob_sentiment(sentence) + self.get_flair_sentiment(sentence))/2\n",
    "        # a bit difficult to understand, but the aim here is to remove the prediction that is of a different polarity\n",
    "        # from the rest (e.g. only take the average of two positive predictions and discount the negative prediction)\n",
    "        elif (vader_positive!=textblob_positive or vader_positive!=flair_positive):\n",
    "            if vader_positive:\n",
    "                if textblob_positive:\n",
    "                    return (self.get_vader_sentiment(sentence) + self.get_textblob_sentiment(sentence))/2\n",
    "                else:\n",
    "                    return (self.get_vader_sentiment(sentence) + self.get_flair_sentiment(sentence))/2\n",
    "            else:\n",
    "                return (self.get_textblob_sentiment(sentence) + self.get_flair_sentiment(sentence))/2\n",
    "        else:\n",
    "            return (self.get_vader_sentiment(sentence) + self.get_textblob_sentiment(sentence) + self.get_flair_sentiment(sentence))/3\n",
    "    def get_all_vadar_sentiment(self):\n",
    "        '''returns all VADER (Valence Aware Dictionary and sEntiment Reasoner) sentiments, which is based off a form of lexicon and rule-based feeling analysis'''\n",
    "        return self.get_all('vader')\n",
    "    def get_all_textblob_sentiment(self):\n",
    "        '''returns all TextBlob sentiments, which is based off noun-phrase extraction, position tracking and more'''\n",
    "        return self.get_all('textblob')\n",
    "    def get_all_flair_sentiment(self):\n",
    "        '''returns all Flair sentiments, which is based off a pre-trained machine learning model from Twitter'''\n",
    "        return self.get_all('flair')\n",
    "    def get_all_average_sentiment(self):\n",
    "        '''returns all average of VADER, TextBlob and Flair sentiments'''\n",
    "        return self.get_all('average')\n",
    "    def get_all_weighted_sentiment(self):\n",
    "        '''returns all weighted average of VADER, TextBlob and Flair sentiments when there are conflicting non-zero sentiments'''\n",
    "        return self.get_all('weighted')\n",
    "    def get_all(self, which):\n",
    "        '''please do not use this, this is the helper function'''\n",
    "        scores = []\n",
    "        if which == 'vader':\n",
    "            for sentence in self.list_of_sentences:\n",
    "                scores += [round(self.get_vader_sentiment(sentence),5)]\n",
    "        elif which == 'textblob':\n",
    "            for sentence in self.list_of_sentences:\n",
    "                scores += [round(self.get_textblob_sentiment(sentence),5)]\n",
    "        elif which == 'flair':\n",
    "            for sentence in self.list_of_sentences:\n",
    "                scores += [round(self.get_flair_sentiment(sentence),5)]\n",
    "        elif which == 'average':\n",
    "            for sentence in self.list_of_sentences:\n",
    "                scores += [round(self.get_average_sentiment(sentence),5)]\n",
    "        elif which == 'weighted':\n",
    "            for sentence in self.list_of_sentences:\n",
    "                scores += [round(self.get_weighted_sentiment(sentence),5)]\n",
    "        return scores\n",
    "    def change_sentences(self,list_of_sentences):\n",
    "        '''enables changing of object's list of sentences on the fly'''\n",
    "        self.list_of_sentences = list_of_sentences\n",
    "        return\n",
    "\n",
    "def get_preprocessed_df(dff):\n",
    "    def convertColons(x):\n",
    "        ex, tick = x.split(':')\n",
    "        if ex=='TSX':\n",
    "            return tick+'.TRT'\n",
    "        elif ex=='TSXV':\n",
    "            return tick+'.TRV'\n",
    "        else:\n",
    "            return tick\n",
    "\n",
    "    def removeStockDuplicates(x):\n",
    "        x = list(set(x))\n",
    "        to_remove = [i.split('.')[0] for i in x if '.' in i]\n",
    "        x = [i for i in x if i not in to_remove]\n",
    "        return sorted(x)\n",
    "    print('Preprocessing Unindexed Data')\n",
    "    # data cleaning\n",
    "    dff['title'] = dff['title'].apply(lambda x: x.split('|')[0])\n",
    "    dff['title'] = dff['title'].apply(lambda x: x.replace('’',\"'\"))\n",
    "    dff['title'] = dff['title'].apply(lambda x: x.replace('”','\"'))\n",
    "    dff['title'] = dff['title'].apply(lambda x: x.replace('“','\"'))\n",
    "    dff['title'] = dff['title'].astype('str')\n",
    "\n",
    "    dff['content'] = dff['content'].apply(lambda x: ' '.join(x))\n",
    "    dff['content'] = dff['content'].apply(lambda x: x.replace('\\\\xa0',' '))\n",
    "    dff['content'] = dff['content'].apply(lambda x: x.replace('’',\"'\"))\n",
    "    dff['content'] = dff['content'].apply(lambda x: x.replace('”','\"'))\n",
    "    dff['content'] = dff['content'].apply(lambda x: x.replace('“','\"'))\n",
    "\n",
    "    dff['stocks_mentioned'] = dff['stocks_mentioned'].apply(lambda x: str(x).replace('\\\\xa0',' '))\n",
    "    dff['stocks_mentioned'] = dff['stocks_mentioned'].apply(lambda x: x.replace('’',\"'\"))\n",
    "    dff['stocks_mentioned'] = dff['stocks_mentioned'].apply(lambda x: x.replace('”','\"'))\n",
    "    dff['stocks_mentioned'] = dff['stocks_mentioned'].apply(lambda x: x.replace('“','\"'))\n",
    "    dff['stocks_mentioned'] = dff['stocks_mentioned'].str[2:-2].str.split(\"', '\").tolist()\n",
    "    dff['stocks_mentioned'] = dff['stocks_mentioned'].apply(lambda x: list(set(x)))\n",
    "    dff['stock_mentioned_in_content'] = dff['content'].apply(lambda x: [i for i in re.findall('\\((.*?\\:.*?)\\)', x) if len(i)<=10])\n",
    "    dff['stock_mentioned_in_content'] = dff['stock_mentioned_in_content'].apply(lambda x: [convertColons(i) for i in x])\n",
    "    dff['stocks_mentioned'] = dff['stock_mentioned_in_content']+dff['stocks_mentioned']\n",
    "    dff=dff.drop(['stock_mentioned_in_content'], axis=1)\n",
    "    dff['stocks_mentioned'] = dff['stocks_mentioned'].apply(lambda x: removeStockDuplicates(x))\n",
    "    dff['stocks_mentioned']=dff['stocks_mentioned'].apply(lambda x: str(x))\n",
    "    \n",
    "    dff['key_points'] = dff['key_points'].apply(lambda x: ' '.join(str(x)[2:-2].split(\"', '\")))\n",
    "    dff['key_points'] = dff['key_points'].apply(lambda x: str(x).replace('\\\\xa0',' '))\n",
    "    dff['key_points'] = dff['key_points'].apply(lambda x: x.replace('’',\"'\"))\n",
    "    dff['key_points'] = dff['key_points'].apply(lambda x: x.replace('”','\"'))\n",
    "    dff['key_points'] = dff['key_points'].apply(lambda x: x.replace('“','\"'))\n",
    "    dff['key_points'] = dff['key_points'].apply(lambda x: \"NaN\")\n",
    "    \n",
    "    summ=list(dff['content'])\n",
    "    def get_summary(idx):\n",
    "        text=summ[idx]\n",
    "        words = word_tokenize(text)\n",
    "        freqTable = dict()\n",
    "        for word in words:\n",
    "            word = word.lower()\n",
    "            if word in stopword:\n",
    "                continue\n",
    "            if word in freqTable:\n",
    "                freqTable[word]+=1\n",
    "            else:\n",
    "                freqTable[word]=1\n",
    "\n",
    "        sentences = sent_tokenize(text)\n",
    "        sentenceValue = dict()\n",
    "        for sentence in sentences:\n",
    "        #     print(sentence)\n",
    "        #     word = word.lower()\n",
    "            for word, freq in freqTable.items():\n",
    "                if word in sentence.lower():  \n",
    "                    if sentence in sentenceValue:\n",
    "                        sentenceValue[sentence]+= freq\n",
    "                    else:\n",
    "                        sentenceValue[sentence]=freq\n",
    "        sumValues = 0\n",
    "        for sentence in sentenceValue:\n",
    "            sumValues += sentenceValue[sentence]\n",
    "        average = int(sumValues / len(sentenceValue))\n",
    "        summary=\"\"\n",
    "        for sentence in sentences:\n",
    "            if (sentence in sentenceValue) and (sentenceValue[sentence]>1.2*average):\n",
    "                summary += \" \" + sentence\n",
    "        if summary == \"\":\n",
    "            summary=sentences[0]\n",
    "        summ[idx]=summary\n",
    "    pool = Pool(processes=8)\n",
    "    print('Summarizing Text')\n",
    "    mapped_summ_values = list(tqdm.tqdm(pool.imap_unordered(get_summary, list(range(len(summ)))), total=len(summ)))\n",
    "    dff['extract_summarizer']=summ\n",
    "    dff['extract_summarizer'] = dff['extract_summarizer'].apply(lambda x: str(x).replace('\\\\xa0',' '))\n",
    "    dff['extract_summarizer'] = dff['extract_summarizer'].apply(lambda x: x.replace(\"', '\",' ')) # ', ' -> ' '\n",
    "    dff['extract_summarizer'] = dff['extract_summarizer'].apply(lambda x: x.replace(\"\\\", '\",' ')) # \", ' -> ' '\n",
    "    dff['extract_summarizer'] = dff['extract_summarizer'].apply(lambda x: x.replace(\"', \\\"\",' ')) # \", ' -> ' '\n",
    "    dff['extract_summarizer'] = dff['extract_summarizer'].apply(lambda x: x.replace(\"[\",'')) # [ ->\n",
    "    dff['extract_summarizer'] = dff['extract_summarizer'].apply(lambda x: x.replace(\"]\",'')) # ] ->\n",
    "    dff['extract_summarizer'] = dff['extract_summarizer'].apply(lambda x: x.replace('’',\"'\")) # ’ -> '\n",
    "    dff['extract_summarizer'] = dff['extract_summarizer'].apply(lambda x: x.replace('”','\"')) # ” -> \"\n",
    "    dff['extract_summarizer'] = dff['extract_summarizer'].apply(lambda x: x.replace('“','\"')) # “ -> \"\n",
    "    \n",
    "    \n",
    "    title=list(dff['title'])\n",
    "    content=list(dff['content'])\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    def lem_text_t(idx):\n",
    "        x=title[idx]\n",
    "        #print(x)\n",
    "        try:\n",
    "            doc = x.replace('TSX:', 'TSX')\n",
    "            doc = nlp(re.sub(r'[^\\w\\s]', '', doc))\n",
    "            text = []\n",
    "            for q in doc:\n",
    "                if q.lemma_ != '-PRON-':\n",
    "                    text+=[q.lemma_]\n",
    "            doc = ' '.join(text)\n",
    "            title[idx]=doc.replace(' s ', ' ')\n",
    "        except:\n",
    "            title[idx]= ''\n",
    "    def lem_text_c(idx):\n",
    "        x=content[idx]\n",
    "        try:\n",
    "            doc = x.replace('TSX:', 'TSX')\n",
    "            doc = nlp(re.sub(r'[^\\w\\s]', '', doc))\n",
    "            text = []\n",
    "            for q in doc:\n",
    "                if q.lemma_ != '-PRON-':\n",
    "                    text+=[q.lemma_]\n",
    "            doc = ' '.join(text)\n",
    "            content[idx]=doc.replace(' s ', ' ')\n",
    "        except:\n",
    "            content[idx]= ''\n",
    "    test = SentimentAnalysis(['this is such a goddamn hassle', 'man... this is one of the best stocks to invest', 'what the hell', \"damn I don't hate this as much as i thought\"])\n",
    "    dff['merged_key_points'] = dff.apply(lambda x: x['extract_summarizer'], axis=1)\n",
    "    kp=list(dff['extract_summarizer'])\n",
    "    def get_pred(idx):\n",
    "        y=[kp[idx]]\n",
    "        test.change_sentences(y)\n",
    "        prediction_results = test.get_all_weighted_sentiment()\n",
    "        kp[idx] = prediction_results[0]\n",
    "    print('Lemmatizing Title Text')\n",
    "    mapped_title_values = list(tqdm.tqdm(pool.imap_unordered(lem_text_t, list(range(len(title)))), total=len(title)))\n",
    "    print('Lemmatizing Title Content')\n",
    "    mapped_content_values = list(tqdm.tqdm(pool.imap_unordered(lem_text_c, list(range(len(content)))), total=len(content)))\n",
    "    print('Analyzing Text Sentiment')\n",
    "    mapped_sent_values = list(tqdm.tqdm(pool.imap_unordered(get_pred, list(range(len(kp)))), total=len(kp)))\n",
    "    dff['lemmatized_title'] = title\n",
    "    dff['lemmatized_content'] = content\n",
    "    dff['predict'] = kp\n",
    "    dff['predict_cat'] = dff['predict'].apply(lambda x: 'Positive' if x>0.2 else ('Negative' if x<-0.2  else 'Neutral'))\n",
    "    \n",
    "    return dff\n",
    "def wrapper(a):\n",
    "    solr = pysolr.Solr('http://ec2-3-132-215-195.us-east-2.compute.amazonaws.com:8983/solr/articleindex', always_commit=True)\n",
    "    latest_date= get_latest_date(solr)\n",
    "    #latest_date=dt.strptime(\"2022/3/26 10:00:00\",\"%Y/%m/%d %H:%M:%S\")\n",
    "    s=latest_date.strftime(\"%d %B %Y\")\n",
    "    prox = get_proxies()\n",
    "    print('Getting All Unindexed Article URLS from '+s)\n",
    "    all_url = update_url_list(prox,latest_date)\n",
    "    print('Getting All Unindexed Article Content')\n",
    "    df= update_content_df(prox,all_url) \n",
    "    df1= get_preprocessed_df(df)\n",
    "    print('Adding Unindexed Articles to Solr Index')\n",
    "    js = df1.to_dict(orient='records')\n",
    "    solr.add(js)\n",
    "    return df1,js"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b84f9b67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "341372b9",
   "metadata": {},
   "source": [
    "### Solr via pysolr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ba704976",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysolr\n",
    "from flask import Flask\n",
    "from flask_cors import CORS\n",
    "from flask_session import Session\n",
    "from flask import request\n",
    "from collections import OrderedDict\n",
    "import json\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "from symspellpy import SymSpell, Verbosity \n",
    "import pkg_resources\n",
    "from nltk.corpus import stopwords \n",
    "import string \n",
    "# import spacy\n",
    "# import re\n",
    "# nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "04a5189e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is for lemmatisation\n",
    "import spacy\n",
    "import re\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c03817c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "solr = pysolr.Solr('http://ec2-3-132-215-195.us-east-2.compute.amazonaws.com:8983/solr/articleindex', always_commit=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0064e175",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n  \"responseHeader\":{\\n    \"zkConnected\":null,\\n    \"status\":0,\\n    \"QTime\":0,\\n    \"params\":{\\n      \"q\":\"{!lucene}*:*\",\\n      \"distrib\":\"false\",\\n      \"df\":\"_text_\",\\n      \"rows\":\"10\",\\n      \"echoParams\":\"all\",\\n      \"rid\":\"solr-node-1-23\"}},\\n  \"status\":\"OK\"}\\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check health of system, whether solr is online\n",
    "solr.ping()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3362a2d9",
   "metadata": {},
   "source": [
    "The below link is a guide for us to phrase queries: <br>\n",
    "https://solr.apache.org/guide/6_6/common-query-parameters.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f51949c1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!pip show Flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc230e",
   "metadata": {},
   "source": [
    "# This Cell must be ran for the solr server to be deployed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212b0c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-10 15:16:15,009 INFO: * Running on http://127.0.0.1:5003/ (Press CTRL+C to quit)\n"
     ]
    }
   ],
   "source": [
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "results =[]\n",
    "\n",
    "def convertToDic(results):\n",
    "    \n",
    "    query_dic ={}\n",
    "\n",
    "    for index in range(0,len(results)):\n",
    "        \n",
    "        query_dic[index]=results.docs[index]\n",
    "    \n",
    "    return query_dic\n",
    "\n",
    "def sortDicbyDate(results):\n",
    "    query_dic ={}\n",
    "\n",
    "    for index in range(0,len(results)):\n",
    "        \n",
    "        query_dic[index]=results.docs[index]\n",
    "    #sorting the dictionary\n",
    "    ordered_dic = OrderedDict(reversed(sorted(query_dic.items(), key = lambda tup: datetime.strptime(tup[1][\"date_published\"][0], '%Y/%m/%d'))))\n",
    "    #reordering the dictionary\n",
    "    ordered_dic={i:v for i,(k,v) in enumerate(ordered_dic.items(), 0)}\n",
    "    return ordered_dic\n",
    "\n",
    "def sortCombinedDictByDate(query_dic):\n",
    "    ordered_dic = OrderedDict(reversed(sorted(query_dic.items(), key = lambda tup: datetime.strptime(tup[1][\"date_published\"][0], '%Y/%m/%d'))))\n",
    "    #reordering the dictionary\n",
    "    ordered_dic={i:v for i,(k,v) in enumerate(ordered_dic.items(), 0)}\n",
    "    return ordered_dic\n",
    "\n",
    "title = 'title:\"\"'\n",
    "\n",
    "def getInputLength(input):\n",
    "    inputlength=len(input.split())\n",
    "    return inputlength\n",
    "\n",
    "def generate_N_grams(text,ngram):\n",
    "    words=[word for word in text.split(\" \") ]  \n",
    "    temp=zip(*[words[i:] for i in range(0,ngram)])\n",
    "    ans=[' '.join(ngram) for ngram in temp]\n",
    "    #print('ngram list is', ans)\n",
    "    return ans\n",
    "\n",
    "def querysyntax(userinput):\n",
    "    return title[:-1]+userinput+title[-1:]\n",
    "#     print(querystring)\n",
    "\n",
    "\n",
    "def DictListUpdate(lis1, lis2):\n",
    "    for aLis1 in lis1:\n",
    "        if aLis1 not in lis2:\n",
    "            lis2.append(aLis1)\n",
    "    return lis2\n",
    "\n",
    "def dictcombine(d1,d2):\n",
    "    count=len(d1)\n",
    "    for key,value in d2.items():\n",
    "        if value not in d1.values():\n",
    "            d1[count] = value\n",
    "            count+=1\n",
    "\n",
    "    return d1\n",
    "\n",
    "def searchfunction(query):\n",
    "    result = solr.search(f'{querysyntax(query)}',fq=[],rows=100,fl=['title','stocks_mentioned','key_points','date_published','url','predict_cat','extract_summarizer'])\n",
    "    return result\n",
    "\n",
    "\n",
    "\n",
    "def twosearch(x,strlen):\n",
    "    query=generate_N_grams(x,strlen-1)\n",
    "    results=searchfunction(query[0])\n",
    "    results2=searchfunction(query[1])\n",
    "    debug = convertToDic(results)\n",
    "    debug2 = convertToDic(results2)\n",
    "    print('two search ran')\n",
    "    return(dictcombine(debug,debug2))\n",
    "    \n",
    "\n",
    "def threesearch(x,strlen):\n",
    "    print('three two search running')\n",
    "    results=twosearch(x,strlen)\n",
    "    if len(results)>=10:\n",
    "        return results \n",
    "    else:\n",
    "        query = generate_N_grams(x,strlen-2)\n",
    "        results=searchfunction(query[0])\n",
    "        results2=searchfunction(query[1])\n",
    "        results3=searchfunction(query[2])\n",
    "        debug = convertToDic(results)\n",
    "        debug2 = convertToDic(results2)\n",
    "        debug3= convertToDic(results3)\n",
    "        print('tthree search ran')\n",
    "        return(dictcombine(dictcombine(debug,debug2),debug3))\n",
    "    \n",
    "        \n",
    "\n",
    "def foursearch(x,strlen):\n",
    "    results = threesearch(x,strlen)\n",
    "    if len(results) >=10:\n",
    "        return results\n",
    "    else:\n",
    "        query = generate_N_grams(x,strlen-3)\n",
    "        results=searchfunction(query[0])\n",
    "        results2=searchfunction(query[1])\n",
    "        results3=searchfunction(query[2])            \n",
    "        results4=searchfunction(query[3])\n",
    "        debug = convertToDic(results)\n",
    "        debug2 = convertToDic(results2)     \n",
    "        debug3 = convertToDic(results3)  \n",
    "        debug4 = convertToDic(results4) \n",
    "        return(dictcombine(dictcombine(dictcombine(debug,debug2),debug3),debug4))\n",
    "        \n",
    "\n",
    "\n",
    "def fivesearch(x,strlen):\n",
    "    results=foursearch(x,strlen)\n",
    "    if len(results)>=10:\n",
    "        return results\n",
    "    else:\n",
    "        \n",
    "        query = generate_N_grams(x,strlen-4)\n",
    "        results=searchfunction(query[0])\n",
    "        results2=searchfunction(query[1])\n",
    "        results3=searchfunction(query[2])            \n",
    "        results4=searchfunction(query[3])\n",
    "        results5=searchfunction(query[4])\n",
    "        debug = convertToDic(results)\n",
    "        debug2 = convertToDic(results2)     \n",
    "        debug3 = convertToDic(results3)  \n",
    "        debug4 = convertToDic(results4) \n",
    "        debug5 = convertToDic(results5) \n",
    "        return(dictcombine(dictcombine(dictcombine(dictcombine(debug,debug2),debug3),debug4),debug5)) \n",
    "\n",
    "    \n",
    "def lem_text(doc): \n",
    "    try: \n",
    "        doc = nlp(re.sub(r'[^\\w\\s]', '', doc)) \n",
    "        text = [] \n",
    "        for q in doc: \n",
    "            if q.lemma_ != '-PRON-': \n",
    "                text+=[q.lemma_] \n",
    "        doc = ' '.join(text)\n",
    "        print('something')\n",
    "        return doc.replace(' s ', ' ') \n",
    "    except: \n",
    "        print(x) \n",
    "        return ''\n",
    "    \n",
    "def remove_stopwords(sentence): \n",
    "    return ' '.join([word for word in sentence.split(' ') if word not in list(set(stopwords.words('english')))])\n",
    "\n",
    " \n",
    "class spellchecker(): \n",
    "    def __init__(self): \n",
    "        self.speller = SymSpell(max_dictionary_edit_distance=3, prefix_length=7) \n",
    "        dictionary_path = pkg_resources.resource_filename( \n",
    "            \"symspellpy\", \"frequency_dictionary_en_82_765.txt\" \n",
    "        ) \n",
    "        self.speller.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "        \n",
    "        df = pd.read_csv('https://www.tsx.com/files/trading/interlisted-companies.txt', delimiter='\\t', header=1).iloc[:,0:2] \n",
    "        df['Symbol'] = df['Symbol'].apply(lambda x: [x.split(':')[0]][0]) \n",
    "        df['Name'] = df['Name'].apply(lambda x: [i for i in x.translate(str.maketrans('', '', string.punctuation)).split(' ')]) \n",
    "        self.stocks_list = list(set(list(df['Symbol']) + list([item for sublist in list(df['Name']) for item in sublist]))) \n",
    "        self.stocks_list = list(map(str.lower,self.stocks_list))\n",
    "         \n",
    "    def correct(self, sentence): \n",
    "        sentence = sentence.split(' ') \n",
    "        corrected = [] \n",
    "        for word in sentence:\n",
    "            if word in self.stocks_list:\n",
    "                corrected+=[word]\n",
    "                continue\n",
    "            try: \n",
    "                corrected += [sorted([str(i).split(', ') for i in self.speller.lookup(word, Verbosity.ALL, max_edit_distance=3)], key = lambda x: int(x[2])**(1/(int(x[1])+1)), reverse=True)[0][0]] \n",
    "            except: \n",
    "                continue \n",
    "        return ' '.join(corrected) \n",
    "speller = spellchecker()\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "@app.route('/',methods=['GET','POST'])\n",
    "def index():\n",
    "    global results\n",
    "            \n",
    "    if request.method == \"POST\": \n",
    "        type_of_query = request.form.get(\"type\")\n",
    "        \n",
    "        if type_of_query ==\"refresh\":\n",
    "            try: \n",
    "                wrapper(1)\n",
    "            except:\n",
    "                return {\"Status\":\"Success\"}\n",
    "            return {\"Status\":\"Success\"}\n",
    "        \n",
    "        original_query=request.form.get(\"query\")\n",
    "        no_spell_correct_query=request.form.get(\"query\")\n",
    "        x= request.form.get(\"query\")\n",
    "        print(\"X: \", x)\n",
    "        print('Original Queried',x)\n",
    "        x=speller.correct(x)\n",
    "        x=lem_text(x)\n",
    "        no_spell_correct_query=lem_text(no_spell_correct_query)\n",
    "#         print(type_of_query)\n",
    "        print('Lem Query is',x)\n",
    "        x=remove_stopwords(x)\n",
    "        no_spell_correct_query=remove_stopwords(no_spell_correct_query)\n",
    "        print(\"Term Queried: \",x)\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "        if type_of_query == \"title\":\n",
    "\n",
    "            results = searchfunction(x)\n",
    "            strlen=getInputLength(x)\n",
    "            if strlen == 1:\n",
    "                print(convertToDic(results))\n",
    "                return(convertToDic(results))\n",
    "\n",
    "            while len(results)<=10 and strlen>1:\n",
    "                print('less than 10 results ')\n",
    "                \n",
    "#                 strlen=getInputLength(x)\n",
    "                \n",
    "                initdebug = convertToDic(results)\n",
    "   \n",
    "\n",
    "                if strlen==2:\n",
    "                    results= dictcombine(initdebug,twosearch(x,strlen))\n",
    "                    print(results)\n",
    "                    return (results) \n",
    "                elif strlen==3:\n",
    "                    results = dictcombine(initdebug,threesearch(x,strlen))\n",
    "                    print(results)\n",
    "                    return (results) \n",
    "                elif strlen == 4:\n",
    "                    results = dictcombine(initdebug,foursearch(x,strlen))\n",
    "                    print(results)\n",
    "                    return (results) \n",
    "                elif strlen == 5:\n",
    "                    results = dictcombine(initdebug,fivesearch(x,strlen))\n",
    "                    print(results)\n",
    "                    return (results) \n",
    "                else:\n",
    "                    #UI need add this case\n",
    "                    return {}\n",
    "#             print(results)\n",
    "            return (convertToDic(results))\n",
    "                \n",
    "            \n",
    "#             print(results)        \n",
    "#             return (results)        \n",
    "        \n",
    "# debug = convertToDic(results)\n",
    "\n",
    "#             print(f'Saw {len(results)} result(s).\\n')\n",
    "#             return convertToDic(results)\n",
    "#             return(convertToDic(results))\n",
    "\n",
    "        if type_of_query == \"sorted_date_title\":\n",
    "            results = solr.search(f'title:{x}',rows=100,fl=['title','stocks_mentioned','key_points','date_published','url','predict_cat','extract_summarizer'],fq=['',''])\n",
    "            print(f'Saw {len(results)} result(s).\\n')\n",
    "            print(sortDicbyDate(results))\n",
    "            return sortDicbyDate(results)\n",
    "        \n",
    "        if type_of_query == \"sorted_date_ticker\":\n",
    "            results = solr.search(f'stocks_mentioned:{original_query}',rows=100,fl=['title','stocks_mentioned','key_points','date_published','url','predict_cat','extract_summarizer'],fq=['',''])\n",
    "            print(\"elif ran\")\n",
    "            print(f'Saw {len(results)} result(s).\\n')\n",
    "            print(sortDicbyDate(results))\n",
    "            return sortDicbyDate(results)\n",
    "        \n",
    "        if type_of_query == \"ticker\":\n",
    "            results = solr.search(f'stocks_mentioned:{original_query}',rows=100,fl=['title','stocks_mentioned','key_points','date_published','url','predict_cat','extract_summarizer'],fq=['',''])\n",
    "            print(\"elif ran\")\n",
    "            print(f'Saw {len(results)} result(s).\\n')\n",
    "            return convertToDic(results)\n",
    "        \n",
    "        if type_of_query == \"checker\":\n",
    "            return speller.correct(original_query)\n",
    "        \n",
    "        if type_of_query == \"sorted_date_title_no_spell\":\n",
    "            results = searchfunction(no_spell_correct_query)\n",
    "            strlen=getInputLength(no_spell_correct_query)\n",
    "            if strlen == 1:\n",
    "                print(sortCombinedDictByDate(convertToDic(results)))\n",
    "                return(sortCombinedDictByDate(convertToDic(results)))\n",
    "\n",
    "            while len(results)<=10 and strlen>1:\n",
    "                print('less than 10 results ')\n",
    "                \n",
    "#                 strlen=getInputLength(x)\n",
    "                \n",
    "                initdebug = convertToDic(results)\n",
    "   \n",
    "\n",
    "                if strlen==2:\n",
    "                    results= dictcombine(initdebug,twosearch(no_spell_correct_query,strlen))\n",
    "                    print(results)\n",
    "                    print(sortCombinedDictByDate(results))\n",
    "                    return (sortCombinedDictByDate(results)) \n",
    "                elif strlen==3:\n",
    "                    results = dictcombine(initdebug,threesearch(no_spell_correct_query,strlen))\n",
    "                    print(sortCombinedDictByDate(results))\n",
    "                    return (sortCombinedDictByDate(results)) \n",
    "                elif strlen == 4:\n",
    "                    results = dictcombine(initdebug,foursearch(no_spell_correct_query,strlen))\n",
    "                    print(sortCombinedDictByDate(results))\n",
    "                    return (sortCombinedDictByDate(results)) \n",
    "                elif strlen == 5:\n",
    "                    results = dictcombine(initdebug,fivesearch(no_spell_correct_query,strlen))\n",
    "                    print(sortCombinedDictByDate(results))\n",
    "                    return (sortCombinedDictByDate(results)) \n",
    "                else:\n",
    "                    #UI need add this case\n",
    "                    return {}\n",
    "#             print(results)\n",
    "            return sortCombinedDictByDate(convertToDic(results))\n",
    "            \n",
    "            \n",
    "            \n",
    "#             results = solr.search(f'title:{no_spell_correct_query}',rows=100,fl=['title','stocks_mentioned','key_points','date_published','url','predict_cat','extract_summarizer'],fq=['',''])\n",
    "#             print(f'Saw {len(results)} result(s).\\n')\n",
    "#             print(sortDicbyDate(results))\n",
    "#             return sortDicbyDate(results)\n",
    "        \n",
    "        if type_of_query ==\"no_spell_correct_query\":\n",
    "            results = searchfunction(no_spell_correct_query)\n",
    "            strlen=getInputLength(no_spell_correct_query)\n",
    "            if strlen == 1:\n",
    "                print(convertToDic(results))\n",
    "                return(convertToDic(results))\n",
    "\n",
    "            while len(results)<=10 and strlen>1:\n",
    "                print('less than 10 results ')\n",
    "                \n",
    "#                 strlen=getInputLength(x)\n",
    "                \n",
    "                initdebug = convertToDic(results)\n",
    "   \n",
    "\n",
    "                if strlen==2:\n",
    "                    results= dictcombine(initdebug,twosearch(no_spell_correct_query,strlen))\n",
    "                    print((results))\n",
    "                    return ((results)) \n",
    "                elif strlen==3:\n",
    "                    results = dictcombine(initdebug,threesearch(no_spell_correct_query,strlen))\n",
    "                    print((results))\n",
    "                    return ((results)) \n",
    "                elif strlen == 4:\n",
    "                    results = dictcombine(initdebug,foursearch(no_spell_correct_query,strlen))\n",
    "                    print((results))\n",
    "                    return ((results)) \n",
    "                elif strlen == 5:\n",
    "                    results = dictcombine(initdebug,fivesearch(no_spell_correct_query,strlen))\n",
    "                    print((results))\n",
    "                    return ((results)) \n",
    "                else:\n",
    "                    #UI need add this case\n",
    "                    return {}\n",
    "#             print(results)\n",
    "            return (convertToDic(results))\n",
    "            \n",
    "\n",
    "    else:\n",
    "        print('else statement ran')\n",
    "        return {'ans':[]}\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5003)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50387da1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
